{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "Extract Syntactic Info using DP"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# ReqCRF\n",
    "\n",
    "by Tooraj Helmi (thelmi@usc.edu)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Introduction\n",
    "ReqCRF is a model based on BILSTM and CRF used to tag requirments based on the grammar explained in the paper. \n",
    "\n",
    "Note: I ran this code locally on Windows. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Configurations"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_embedding = False\n",
    "parse_data = False\n",
    "encoding = ['word', 'pos', 'dep'] # word, pos, dep\n",
    "tag = 'TAG' # name of the tag column \n",
    "max_len = 50\n",
    "apps = [ 'Trading', 'TicTacToe','WordGuess', 'News', 'Food Delivery', 'Calendar', 'Bank']\n",
    "data_path = '../data/'\n",
    "use_dp = True\n",
    "embedding_dim = { 'word' : 10, 'pos': 3, 'dep': 3}"
   ]
  },
  {
   "source": [
    "## Parsing\n",
    "\n",
    "If parse_data is set to True, the following code will parse a plain text requirement file, break it down to words and POS tags and make it ready for applying NER tags. \n",
    "Note: Make sure to rename input and output files to match you need."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "     APP REQ SENT       WORD    LEMMA    POS        DEP  TAG\n0   News   0    0       When     when    ADV     advmod  NaN\n1   News   0    0        the      the    DET        det  NaN\n2   News   0    0       user     user   NOUN      nsubj  NaN\n3   News   0    0   launches   launch   VERB      advcl  NaN\n4   News   0    0        app      app   NOUN       dobj  NaN\n5   News   0    0        for      for    ADP       prep  NaN\n6   News   0    0        the      the    DET        det  NaN\n7   News   0    0      first    first    ADJ       amod  NaN\n8   News   0    0       time     time   NOUN       pobj  NaN\n9   News   0    0          ,        ,  PUNCT      punct  NaN\n10  News   0    0         he   -PRON-   PRON  nsubjpass  NaN\n11  News   0    0     should   should   VERB        aux  NaN\n12  News   0    0         be       be    AUX    auxpass  NaN\n13  News   0    0  presented  present   VERB       ROOT  NaN\n14  News   0    0       with     with    ADP       prep  NaN\n15  News   0    0          a        a    DET        det  NaN\n16  News   0    0    welcome  welcome    ADJ       amod  NaN\n17  News   0    0    message  message   NOUN       pobj  NaN\n18  News   0    0          ,        ,  PUNCT      punct  NaN\n19  News   0    0          a        a    DET        det  NaN\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize \n",
    "from nltk import RegexpTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords \n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "if parse_data:\n",
    "    reqset = pd.read_csv(data_path + 'Reqset-train.csv', names=[\"SENT\", \"APP\", \"REQ\"], skiprows=range(1))\n",
    "    reqset.head(10)\n",
    "\n",
    "    reqs = reqset[\"REQ\"].to_list()\n",
    "    n_req = len(reqs); n_req\n",
    "    \n",
    "    sents = []\n",
    "    for  req in reqs:\n",
    "        for sent in req.split('.'):\n",
    "            sents.append([reqs.index(req), sent])\n",
    "    n_sent = len(sents); n_sent\n",
    "    \n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    dataset = pd.DataFrame(columns=['APP', 'REQ', 'SENT', 'WORD', 'LEMMA', 'POS', 'DEP', 'TAG']) \n",
    "    for sent in sents:\n",
    "        doc = nlp(sent[1])\n",
    "        for token in doc:\n",
    "            dataset = dataset.append({'APP': reqset.at[sent[0], 'APP'], 'REQ': sent[0], \n",
    "                'SENT': sents.index(sent), 'WORD': token.text, 'LEMMA': token.lemma_, \n",
    "                'POS': token.pos_, 'DEP': token.dep_}, ignore_index = True)\n",
    "\n",
    "    dataset.to_csv(data_path +  \"reqset-train-tagged-new.csv\")\n",
    "    print(dataset.head(20))"
   ]
  },
  {
   "source": [
    "## Load Data\n",
    "Loads the tagged dataset "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<bound method DataFrame.count of        SENT      WORD   POS     DEP      TAG\n0       0.0      When   ADV  advmod  APP-IFF\n1       0.0       the   DET     det        O\n2       0.0      user  NOUN   nsubj  APP-ENT\n3       0.0  launches  VERB   advcl  APP-ACT\n4       0.0       app  NOUN    dobj  APP-ENT\n...     ...       ...   ...     ...      ...\n3573  166.0       the   DET     det        O\n3574  166.0  detailed   ADJ    dobj  APP-ENT\n3575  166.0        of   ADP    prep        O\n3576  166.0       the   DET     det        O\n3577  166.0    events  NOUN    pobj  APP-ENT\n\n[3578 rows x 5 columns]>\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv(data_path + \"reqset-train-tagged.csv\", encoding=\"latin1\")\n",
    "data = data.fillna(value=np.NaN)\n",
    "data = data.dropna(thresh=2)\n",
    "data = data[data[tag].notnull()]\n",
    "data = data[data['APP'].isin(apps)]\n",
    "data = data[[\"SENT\", \"WORD\", \"POS\", \"DEP\", tag]]\n",
    "data.tail(10)\n",
    "print(data.count)"
   ]
  },
  {
   "source": [
    "## Preprocessing\n",
    "The following sections apply indexes to works and labels. Optionally is can also apply GloVe embbeding."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WORD count: 638\nPOS count: 16\nDEP count: 44\nTAG count: 9\nTAGS: ['APP-QLF', 'O', 'APP-IFF', 'APP-ENT', 'APP-POS', 'APP-SHO', 'APP-FIX', 'APP-INP', 'APP-ACT']\n"
     ]
    }
   ],
   "source": [
    "words = [w.lower() for w in list(set(data[\"WORD\"].values))]\n",
    "n_words = len(words); \n",
    "\n",
    "tags = list(set(data[tag].values))\n",
    "n_tags = len(tags); \n",
    "\n",
    "pos = list(set(data['POS'].values))\n",
    "n_pos = len(pos); \n",
    "\n",
    "deps = list(set(data['DEP'].values))\n",
    "n_deps = len(deps); \n",
    "\n",
    "print('WORD count:', n_words)\n",
    "print('POS count:', n_pos)\n",
    "print('DEP count:', n_deps)\n",
    "print('TAG count:', n_tags)\n",
    "\n",
    "print('TAGS:', tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceGetter(object):  \n",
    "    def __init__(self, data, tag):\n",
    "        self.n_sent = 95\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, p, d, t) for w, p, d, t in zip(s[\"WORD\"].values.tolist(),\n",
    "                                                           s[\"POS\"].values.tolist(),\n",
    "                                                           s[\"DEP\"].values.tolist(),\n",
    "                                                           s[tag].values.tolist())]\n",
    "        self.grouped = self.data.groupby(\"SENT\").apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "    \n",
    "    def get_next(self):\n",
    "        try:\n",
    "            s = self.grouped[self.n_sent]\n",
    "            self.n_sent += 1\n",
    "            return s\n",
    "        except:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[(' ', 'SPACE', nan, 'O'), ('User', 'PROPN', 'nsubj', 'APP-ENT'), ('can', 'VERB', 'aux', 'APP-IFF'), ('press', 'VERB', 'ROOT', 'APP-INP'), ('enter', 'VERB', 'dobj', 'APP-ENT'), ('to', 'PART', 'aux', 'O'), ('go', 'VERB', 'advcl', 'APP-ACT'), ('back', 'ADV', 'advmod', 'O'), ('to', 'ADP', 'prep', 'O'), ('the', 'DET', 'det', 'O'), ('main', 'ADJ', 'amod', 'APP-ENT'), ('menu', 'NOUN', 'pobj', 'APP-ENT')]\n"
     ]
    }
   ],
   "source": [
    "getter = SentenceGetter(data, tag)\n",
    "sentences = getter.sentences\n",
    "sent = getter.get_next()\n",
    "print(sent)"
   ]
  },
  {
   "source": [
    "### Glove Embedding\n",
    "If ext_embedding is True, and the glove.6B.50d.txt is available in the data folder will embed tokens"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "embeddings_index = {}\n",
    "\n",
    "if ext_embedding:\n",
    "    with open(data_path + 'glove.6B.50d.txt', 'r', encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split(' ')\n",
    "            word = values[0] ## The first entry is the word\n",
    "            coefs = np.asarray(values[1:], dtype='float32') ## These are the vecotrs representing the embedding for the word\n",
    "            embeddings_index[word] = coefs\n",
    "        f.close()\n",
    "    print('GloVe data loaded')\n",
    "    print(embeddings_index[\"'s\"])"
   ]
  },
  {
   "source": [
    "## Create Indexes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "# Sents:  150\n# Unique Words:  560\n# Unique POS:  16\n# Unique DEPS:  44\n# Unique TAGS:  9\n"
     ]
    }
   ],
   "source": [
    "word2Idx = {w: i + 1 for i, w in enumerate(words)}\n",
    "pos2Idx = {p: i for i, p in enumerate(pos)}\n",
    "dep2Idx = {p: i for i, p in enumerate(deps)}\n",
    "tag2Idx = {t: i for i, t in enumerate(tags)}\n",
    "\n",
    "print('# Sents: ', len(sentences))\n",
    "print('# Unique Words: ', len(word2Idx))\n",
    "print('# Unique POS: ', len(pos2Idx))\n",
    "print('# Unique DEPS: ', len(dep2Idx))\n",
    "print('# Unique TAGS: ', len(tag2Idx))"
   ]
  },
  {
   "source": [
    "## Generating X and Y\n",
    "To following sections will produce necessary data for training. First one uses GloVe embedding and second one does not"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "\n",
    "def encode_with_embedding(sentences):\n",
    "    max_words = 40\n",
    "    embedding_dim = embeddings_index.get('a').shape[0]\n",
    "\n",
    "    n_row = len(sentences)\n",
    "    n_word_dim = embedding_dim + n_pos\n",
    "    X = np.zeros((n_row, n_word_dim * max_words))\n",
    "    Y = []\n",
    "    for sent_idx, sent in enumerate(sentences):\n",
    "        Y_sent = np.tile(to_categorical(tag2idx[\"O\"], num_classes=n_tags), (n_word_dim * max_words, 1))\n",
    "        for word_idx, w in enumerate(w for w in sent[:max_words]):\n",
    "            embedding_vector = embeddings_index.get(w[0].lower()) \n",
    "            pos_vector = to_categorical(pos2Idx[w[1]], num_classes=n_pos) \n",
    "            word_encoded = np.append(embedding_vector, pos_vector)\n",
    "\n",
    "            for i in range(n_word_dim):   \n",
    "                X[sent_idx, i + word_idx * n_word_dim] = word_encoded[i]\n",
    "                Y_sent[i + word_idx * n_word_dim] = to_categorical(tag2idx[w[2]], num_classes=n_tags)\n",
    "        Y.append(Y_sent)\n",
    "\n",
    "    print(X.shape)\n",
    "    print(Y[0].shape)\n",
    "    print(len(Y))\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-9-336b4d0fbfef>, line 6)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-336b4d0fbfef>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    X = X.append[[word2idx[w[0].lower()] for w in s] for s in sentences]\u001b[0m\n\u001b[0m                                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "def encode_no_embedding(sentences):\n",
    "    if 'word' in encoding:\n",
    "        X = X.append[[word2idx[w[0].lower()] for w in s] for s in sentences]\n",
    "    if  encoding == 'pos':\n",
    "        X = [[pos2Idx[w[1]] for w in s] for s in sentences]\n",
    "    X = pad_sequences(maxlen=max_len, sequences=X, padding=\"post\", value=0) \n",
    "\n",
    "    Y = [[tag2idx[w[2]] for w in s] for s in sentences]\n",
    "    Y = pad_sequences(maxlen=max_len, sequences=Y, padding=\"post\", value=tag2idx[\"O\"])\n",
    "    Y = [to_categorical(i, num_classes=n_tags) for i in Y]\n",
    "    \n",
    "    print('X:', X.shape)\n",
    "    print('Y:',Y[0].shape)\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "source": [
    "## Define Model\n",
    "Model includes 3 layers:\n",
    "1. embedding\n",
    "2. BILSTM\n",
    "3. Linear Chain CRF"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-9f90da9bd9db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTimeDistributed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBidirectional\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConcatenate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras_contrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCRF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.models import Model, Input, Sequential\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional, Concatenate\n",
    "from keras_contrib.layers import CRF\n",
    "\n",
    "input = Input(shape=(max_len,))\n",
    "\n",
    "# model = Embedding(input_dim=n_words + 1, output_dim=20,\n",
    "#                   input_length=input_len, mask_zero=True, trainable=False)(input) \n",
    "# model = Bidirectional(LSTM(units=50, return_sequences=True,\n",
    "#                            recurrent_dropout=0.1))(model) \n",
    "# model = TimeDistributed(Dense(50, activation=\"relu\"))(model) \n",
    "# crf = CRF(n_tags) \n",
    "# out = crf(model) \n",
    "\n",
    "embeddings = []\n",
    "p\n",
    "if 'word' in encoding:\n",
    "    embeddings.append(Embedding(input_dim=n_words + 1, output_dim=embedding_dim['word'],\n",
    "        input_length=input_len, mask_zero=True, trainable=True)(input))\n",
    "\n",
    "if 'pos' in encoding:\n",
    "    embeddings.append(Embedding(input_dim=n_pos + 1, output_dim=embedding_dim['pos'],\n",
    "        input_length=input_len, mask_zero=True, trainable=True)(input))\n",
    "\n",
    "if 'dep' in encoding:\n",
    "    embeddings.append(Embedding(input_dim=n_deps + 1, output_dim=embedding_dim['dep'],\n",
    "        input_length=input_len, mask_zero=True, trainable=True)(input)) \n",
    "\n",
    "model = Concatenate(axis=1)(embeddings)\n",
    "model = Bidirectional(LSTM(units=50, return_sequences=True,\n",
    "                           recurrent_dropout=0.1))(model) \n",
    "model = TimeDistributed(Dense(50, activation=\"relu\"))(model) \n",
    "crf = CRF(n_tags) \n",
    "out = crf(model) \n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "embedding_dim['word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(input, out)\n",
    "model.compile(optimizer=\"rmsprop\", loss=crf.loss_function, metrics=[crf.accuracy])\n",
    "model.summary()"
   ]
  },
  {
   "source": [
    "## Train the Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "if use_embedding:\n",
    "    X, Y = encode_with_embedding(sentences)\n",
    "else:\n",
    "    X, Y = encode_no_embedding(sentences)\n",
    "\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, Y, test_size=0.001)\n",
    "# X_tr = X\n",
    "# Y_tr = Y\n",
    "# print(X_tr.shape, X_te.shape, y_tr[0].shape)\n",
    "history = model.fit(X_tr, np.array(y_tr), batch_size=1, epochs=50, validation_split=0.1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = pd.DataFrame(history.history)\n",
    "print(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.plot(hist[\"crf_viterbi_accuracy\"])\n",
    "plt.plot(hist[\"val_crf_viterbi_accuracy\"])\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "## Testing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(data_path + \"reqset-test-tagged.csv\", encoding=\"latin1\")\n",
    "data = data.fillna(value=np.NaN)\n",
    "data = data.dropna(thresh=2)\n",
    "data = data[data[tag].notnull()]\n",
    "data = data[[\"SENT\", \"WORD\", \"POS\", tag]]\n",
    "# print(data.tail(10))\n",
    "print(data.count)\n",
    "\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, Y, test_size=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "test_pred = model.predict(X_tr, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<style type=\"text/css\">\n",
    "table.dataframe td, table.dataframe th {\n",
    "    border: 1px  black solid !important;\n",
    "  color: black !important;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "source": [
    "## Model Performance"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "idx2tag = {i: w for w, i in tag2idx.items()}\n",
    "\n",
    "def pred2label(pred):\n",
    "    out = []\n",
    "    for pred_i in pred:\n",
    "        out_i = []\n",
    "        for p in pred_i:\n",
    "            p_i = np.argmax(p)\n",
    "            out_i.append(idx2tag[p_i].replace(\"PAD\", \"O\"))\n",
    "        out.append(out_i)\n",
    "    return out   \n",
    "\n",
    "pred_labels = pred2label(test_pred)\n",
    "test_labels = pred2label(y_tr)\n",
    "\n",
    "f1_micro = f1_score(test_labels, pred_labels, average='micro')\n",
    "f1_macro = f1_score(test_labels, pred_labels, average='macro')\n",
    "f1_weighted = f1_score(test_labels, pred_labels, average='weighted')\n",
    "print(\"Micro F1-score: {:.2%}\".format(f1_micro))\n",
    "print(\"Macro F1-score: {:.2%}\".format(f1_macro))\n",
    "print(\"Weighted F1-score: {:.2%}\".format(f1_weighted))\n",
    "print(classification_report(test_labels, pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 2\n",
    "p = model.predict(np.array([X_tr[i]]))\n",
    "p = np.argmax(p, axis=-1)\n",
    "true = np.argmax(y_tr[i], -1)\n",
    "print(\"{:15}||{:5}||{}\".format(\"Word\", \"True\", \"Pred\"))\n",
    "print(30 * \"=\")\n",
    "for w, t, pred in zip(X_tr[i], true, p[0]):\n",
    "    if w != 0:\n",
    "        print(\"{:15}: {:5} {}\".format(words[w-1], tags[t], tags[pred]))"
   ]
  },
  {
   "source": [
    "## Predict Labels for a Requirement"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = [\"if\", \"user\", \"picks\", \"the\", \"first\", \"choice\", \",\", \"he\", \"should\", \"be\", \"shown\", \"a\", \"white\", \"baloon\", \"below\", \"the\", \"text\"]\n",
    "x_test_sent = pad_sequences(sequences=[[word2idx.get(w, 0) for w in test_sentence]],\n",
    "                            padding=\"post\", value=0, maxlen=max_len)\n",
    "\n",
    "p = model.predict(np.array([x_test_sent[0]]))\n",
    "p = np.argmax(p, axis=-1)\n",
    "print(\"{:15}||{}\".format(\"Word\", \"Prediction\"))\n",
    "print(30 * \"=\")\n",
    "for w, pred in zip(test_sentence, p[0]):\n",
    "    print(\"{:15}: {:5}\".format(w, tags[pred]))"
   ]
  }
 ]
}