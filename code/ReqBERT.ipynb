{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ReqBERT.ipynb","provenance":[{"file_id":"18D9CtKc4mGJPUJNzXUr5NQWAFt9_OL_K","timestamp":1603332298288},{"file_id":"1m-C-bWYvhej2eoRfkk3sKBdQHWRWw_Xu","timestamp":1603321367218},{"file_id":"1Jya2mTThi_04fkw2m8O2nHM6StOKhVZ0","timestamp":1589470674801},{"file_id":"1Y4o3jh3ZH70tl6mCd76vz_IxX23biCPP","timestamp":1582810711026},{"file_id":"1dcucJvUOx6kdopjhVIwIdpby6VXhnvns","timestamp":1575478354980},{"file_id":"1ywsvwO6thOVOrfagjjfuxEf6xVRxbUNO","timestamp":1575307876986},{"file_id":"1f_snPs--PVYgZJwT3GwjxqVALFJ0T2-y","timestamp":1556493831452}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3.7.8 64-bit","metadata":{"interpreter":{"hash":"57baa5815c940fdaff4d14510622de9616cae602444507ba5d0b6727c008cbd6"}}},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"ef65b34c044249898b0b86278175367b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_e422e09a72564a29a3ea61a61c011faf","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_57aab739f6754331b15934c19f1c7b94","IPY_MODEL_8460afa17a2645c5bba53405731885f3"]}},"e422e09a72564a29a3ea61a61c011faf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"57aab739f6754331b15934c19f1c7b94":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_5ff0e7a945a846668072388c05eeae62","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":433,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":433,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f72b12d079a74e8086db8d56edf7917b"}},"8460afa17a2645c5bba53405731885f3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_bc7cff93cabc4e7fa68ca4e9723a6ccf","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 433/433 [00:23&lt;00:00, 18.5B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_fa84b8ae16be4517a497fb7f56396f88"}},"5ff0e7a945a846668072388c05eeae62":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"f72b12d079a74e8086db8d56edf7917b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"bc7cff93cabc4e7fa68ca4e9723a6ccf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"fa84b8ae16be4517a497fb7f56396f88":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6087355cfc5c4e5f832c3d50ec03e92c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_ca03d06bc88644cc89175c0e692365c6","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_5065180a3fce4e788ee6d3e193b655ca","IPY_MODEL_41980f25ce5e485cb1b7a3c37d0db018"]}},"ca03d06bc88644cc89175c0e692365c6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5065180a3fce4e788ee6d3e193b655ca":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_0cdb153fffb04401af6be8bc3b49a190","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":440473133,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":440473133,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_3a5b972f4616451eb150ecf0b13f1033"}},"41980f25ce5e485cb1b7a3c37d0db018":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_f32641cac54d45ec8ead8201f4d1aaa8","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 440M/440M [00:06&lt;00:00, 65.2MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6fb8cb633526461183b57a51794716c8"}},"0cdb153fffb04401af6be8bc3b49a190":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"3a5b972f4616451eb150ecf0b13f1033":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f32641cac54d45ec8ead8201f4d1aaa8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"6fb8cb633526461183b57a51794716c8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e0c2db4225b1480f88c87f56bb709d40":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_9564a352a0e34f10935db3e6b9c3fa3e","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_2a5ed12259c44522b11521cd2a1ef45a","IPY_MODEL_3165271833e146d2945567a3b265d346"]}},"9564a352a0e34f10935db3e6b9c3fa3e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2a5ed12259c44522b11521cd2a1ef45a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_dd2a993fb55845a6a012873a02ceae39","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":20,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":20,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f07a9f58a3c44c75b5aeb56ca9f81513"}},"3165271833e146d2945567a3b265d346":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_9dabe991be8c4d188e52e3029c9524bb","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 20/20 [23:21&lt;00:00, 70.10s/it]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_dc24c8da5c2b4ae4bdecc0e2e83ec517"}},"dd2a993fb55845a6a012873a02ceae39":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"f07a9f58a3c44c75b5aeb56ca9f81513":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9dabe991be8c4d188e52e3029c9524bb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"dc24c8da5c2b4ae4bdecc0e2e83ec517":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"EKOTlwcmxmej"},"source":["# ReqBERT \n","by Tooraj Helmi (thelmi@usc.edu)"]},{"cell_type":"markdown","metadata":{"id":"IP8eXq3IJbZD"},"source":["## Introduction\n","This model a finetuned BERT used to tag requirments based on the grammar explained in the paper. \n","\n","Note: I used Colab for running this model. \n"]},{"cell_type":"markdown","metadata":{"id":"RX_ZDhicpHkV"},"source":["## Configurations"]},{"cell_type":"code","metadata":{"id":"t6HgueJv1KLh","executionInfo":{"status":"ok","timestamp":1606357382389,"user_tz":480,"elapsed":1114,"user":{"displayName":"Tooraj Helmi","photoUrl":"","userId":"13522793036712981936"}}},"source":["import warnings\n","\n","_debug_ = False\n","_root_path_ = '/content/drive/MyDrive/Research/Automata/RetBERT/'\n","_debug_ = False\n","_tag_ = 'TAG7'\n","_max_len_ = 50\n","_batch_size_ = 32\n","_learning_rate_ = 5e-5\n","_epochs_ = 20\n","apps = ['Trading', 'TicTacToe', 'WordGuess', 'News', 'Food Delivery', 'Calendar', 'Bank'] #, 'TimeCard', 'Alarm']\n","\n","warnings.filterwarnings('ignore')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nSU7yERLP_66"},"source":["## 1.1. Select GPU\n","We need to have a GPU available to run this model"]},{"cell_type":"code","metadata":{"id":"DEfSbAA4QHas","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606357034669,"user_tz":480,"elapsed":6012,"user":{"displayName":"Tooraj Helmi","photoUrl":"","userId":"13522793036712981936"}},"outputId":"9402c76a-b876-40d1-f426-f8c6aedceba4"},"source":["import tensorflow as tf\n","\n","# Get the GPU device name.\n","device_name = tf.test.gpu_device_name()\n","\n","# The device name should look like the following:\n","if device_name == '/device:GPU:0':\n","    print('Found GPU at: {}'.format(device_name))\n","else:\n","    raise SystemError('GPU device not found')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oYsV4H8fCpZ-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606357036719,"user_tz":480,"elapsed":8057,"user":{"displayName":"Tooraj Helmi","photoUrl":"","userId":"13522793036712981936"}},"outputId":"56f0a0b1-26e9-40e5-a870-120c909d5960"},"source":["import torch\n","\n","# If there's a GPU available...\n","if torch.cuda.is_available():    \n","\n","    # Tell PyTorch to use the GPU.    \n","    device = torch.device(\"cuda\")\n","\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","# If not...\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2ElsnSNUridI"},"source":["## 1.2. Installing Packages\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"0NmMdkZO8R6q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606357049099,"user_tz":480,"elapsed":20431,"user":{"displayName":"Tooraj Helmi","photoUrl":"","userId":"13522793036712981936"}},"outputId":"7ec78838-a1d0-4f0c-ab0d-6c010c13f55c"},"source":["!pip install transformers\n","!pip install seqeval"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"guw6ZNtaswKc"},"source":["# Loading & Parse\n"]},{"cell_type":"markdown","metadata":{"id":"4JrUHXms16cn"},"source":["## 2.1. Load"]},{"cell_type":"code","metadata":{"id":"52-dhlh3aJBI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606357073681,"user_tz":480,"elapsed":45008,"user":{"displayName":"Tooraj Helmi","photoUrl":"","userId":"13522793036712981936"}},"outputId":"3379c2f2-81ff-4356-a64f-29bc4ae4ad28"},"source":["from google.colab import drive\n","drive.mount(\"/content/drive\", force_remount=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"izsz3166ng-C","executionInfo":{"status":"ok","timestamp":1606357073682,"user_tz":480,"elapsed":45007,"user":{"displayName":"Tooraj Helmi","photoUrl":"","userId":"13522793036712981936"}}},"source":["import pandas as pd\n","import numpy as np\n","\n","import pandas as pd\n","import numpy as np\n","\n","def load_data(path, tag, all_apps):\n","  data = pd.read_csv(path, encoding=\"latin1\")\n","  data = data.fillna(value=np.NaN)\n","  data = data.dropna(thresh=2)\n","  data = data[data[tag].notnull()]\n","  if not all_apps:\n","    data = data[data['APP'].isin(apps)]\n","  data = data[[\"SENT\", \"WORD\", \"POS\", tag]]\n","  print(data.shape)\n","  print()\n","  return data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DLaomosQwF5Z"},"source":["Load Tagged Reqset"]},{"cell_type":"markdown","metadata":{"id":"I6A5kw2UBXsQ"},"source":["## 2.3. Parse Dataset"]},{"cell_type":"code","metadata":{"id":"CkAWuZTalRjy","executionInfo":{"status":"ok","timestamp":1606357074483,"user_tz":480,"elapsed":45807,"user":{"displayName":"Tooraj Helmi","photoUrl":"","userId":"13522793036712981936"}}},"source":["import math\n","import seaborn as sns\n","from matplotlib import pyplot\n","\n","def parse_dataset(data, show_info=True):\n","  tokens = []\n","  token_labels = []\n","  sentences = []\n","  all_tokens = []\n","  unique_tokens = set()\n","  labels = []\n","  unique_labels = set()\n","  label_map = {}\n","\n","  sent_idx = data['SENT'].iloc[0]\n","\n","  for index, row in data.iterrows():                                                                \n","    if row['SENT'] != sent_idx:\n","      sentences.append(tokens)\n","      labels.append(token_labels)           \n","\n","      tokens = []\n","      token_labels = []\n","\n","    sent_idx = row['SENT']     \n","\n","    tokens.append(row['WORD'])\n","    token_labels.append(row[_tag_])\n","    unique_labels.add(row[_tag_])\n","\n","    all_tokens.append(row['WORD'])\n","    unique_tokens.add(row['WORD'])\n","\n","    # BERT can toeknize each of the NLTK tokens into multiple tokens, e.g., \n","    # 'non-admin' => ', non, -, admin, -. So we need to repeat the tags for\n","    # all of these extra tokens\n","    bert_tokens = tokenizer.encode(row['WORD'], add_special_tokens = False)\n","    for token_id in bert_tokens:\n","      if tokenizer.ids_to_tokens[token_id][0:2] != '##':\n","        token_labels.append(row[_tag_])\n","  \n","  # Last sentence\n","  sentences.append(tokens)\n","  labels.append(token_labels) \n","\n","  for (i, label) in enumerate(unique_labels):\n","      label_map[label] = i\n","  \n","  if show_info:\n","    print('Label Map: ', label_map)\n","\n","    #Plot label dist.\n","    freq = pd.DataFrame(columns=['label','count'])\n","    flat_labels = [l for s in labels for l in s]\n","\n","    for l in label_map:\n","      freq = freq.append({'label': l, 'count': flat_labels.count(l)}, ignore_index=True)\n","\n","    #pyplot.subplots(figsize=[20,5])\n","    plot = sns.barplot(x=freq['label'], y=freq['count'])\n","    for item in plot.get_xticklabels():\n","      item.set_rotation(90)\n","      \n","  return sentences, all_tokens, unique_tokens, labels, unique_labels, label_map"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SIDfAzc9C2wl"},"source":["# Data Prep"]},{"cell_type":"markdown","metadata":{"id":"uEyKypVX9-y6"},"source":["## 3.1. Max Length"]},{"cell_type":"code","metadata":{"id":"2bBdb3pt8LuQ","executionInfo":{"status":"ok","timestamp":1606357074808,"user_tz":480,"elapsed":46130,"user":{"displayName":"Tooraj Helmi","photoUrl":"","userId":"13522793036712981936"}}},"source":["from transformers import BertTokenizer\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","def analyze():\n","  lengths = []\n","\n","  for sen in sentences:\n","      sen = ' '.join(sen)\n","\n","      encoded_sent = tokenizer.encode(sen, add_special_tokens = True) \n","      lengths.append(len(encoded_sent))\n","\n","  print('Min length: {:,} tokens'.format(min(lengths)))\n","  print('Max length: {:,} tokens'.format(max(lengths)))\n","  print('Median length: {:,} tokens'.format(int(np.median(lengths))))\n","  print()\n","  \n","  sns.set(style='darkgrid')\n","\n","  sns.set(font_scale=1.5)\n","  plt.rcParams[\"figure.figsize\"] = (10,5)\n","\n","  sns.distplot(lengths, kde=False, rug=False)\n","\n","  plt.title('Sentence Lengths')\n","  plt.xlabel('Sentence Length')\n","  plt.ylabel('# of Sentences')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"91PCHUVoEw3L"},"source":["## 3.2. Tokenize"]},{"cell_type":"markdown","metadata":{"id":"tIWAoWL2RK1p"},"source":["The `tokenizer.encode_plus` applies the following:\n","\n","1. Split unknown words into subwords.\n","2. Add the special `[CLS]` and `[SEP]` tokens.\n","3. Map the tokens to their IDs.\n","4. Pad or truncate all sentences to the same length (we chose 50).\n","5. Create the attention masks which explicitly differentiate real tokens from `[PAD]` tokens (so that the self-attention mechanism in BERT ignores the `[PAD]` tokens).\n"]},{"cell_type":"code","metadata":{"id":"8rZLGEABFPez","executionInfo":{"status":"ok","timestamp":1606357074809,"user_tz":480,"elapsed":46130,"user":{"displayName":"Tooraj Helmi","photoUrl":"","userId":"13522793036712981936"}}},"source":["def tokenize(sentences):\n","  input_ids = []\n","  attention_masks = []\n","  \n","  for sent in sentences:\n","      sent_str = ' '.join(sent)\n","      encoded_dict = tokenizer.encode_plus(\n","        sent_str,                  \n","        add_special_tokens = True, \n","        truncation = True,\n","        max_length = _max_len_,           \n","        pad_to_max_length = True,\n","        return_attention_mask = True,  \n","        return_tensors = 'pt')\n","        \n","      input_ids.append(encoded_dict['input_ids'][0])\n","      \n","      attention_masks.append(encoded_dict['attention_mask'][0])\n","  if _debug_:\n","    print('Original: ', sentences[1])\n","    print('Token IDs:', input_ids[1])\n","    print('Masks:', attention_masks[1])\n","\n","  return input_ids, attention_masks"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ze5ljJQuFkKL"},"source":["## 3.3. Map to Labels\n","BERT tokenizer can break the words if they are not located in WordPiece vocabulary. Therefore, we need to apply the original word labels to all of the new pieces"]},{"cell_type":"code","metadata":{"id":"kDCfJPLjFn0r","executionInfo":{"status":"ok","timestamp":1606357074809,"user_tz":480,"elapsed":46128,"user":{"displayName":"Tooraj Helmi","photoUrl":"","userId":"13522793036712981936"}}},"source":["def map_to_labels():\n","  new_labels = []\n","  null_label_id = -100\n","  sent_idx = 0\n"," \n","  for (sen, orig_labels) in zip(input_ids, labels):\n","      if (_debug_):\n","        print('============================DEBUG==============================')\n","        print(sentences[sent_idx])\n","        print(sen)\n","        print(orig_labels)\n","        for t in sen: \n","          print(tokenizer.ids_to_tokens[t.numpy().item()])\n","      padded_labels = []\n","      orig_labels_i = 0 \n","\n","      for token_id in sen:\n","          token_id = token_id.numpy().item()\n","\n","          # If `[PAD]`, `[CLS]`, or `[SEP]`...\n","          if (token_id == tokenizer.pad_token_id) or \\\n","              (token_id == tokenizer.cls_token_id) or \\\n","              (token_id == tokenizer.sep_token_id):\n","              \n","              padded_labels.append(null_label_id)\n","\n","          # If the token string starts with \"##\"...\n","          elif tokenizer.ids_to_tokens[token_id][0:2] == '##':\n","              padded_labels.append(null_label_id)\n","\n","          # If it's not any of the above...\n","          else:\n","              if (_debug_):\n","                print(orig_labels_i)\n","              label_str = orig_labels[orig_labels_i]\n","              \n","              # This might happen if a test label is not training\n","              if label_str not in label_map:\n","                label_map[label_str] = len(label_map)\n","\n","              padded_labels.append(label_map[label_str])\n","              orig_labels_i += 1\n","      sent_idx += 1;\n","      assert(len(sen) == len(padded_labels))    \n","      new_labels.append(padded_labels)\n","  return new_labels"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NRcNF-_0DYgO"},"source":["## 3.4. Prediction"]},{"cell_type":"code","metadata":{"id":"JfwSnqutDXZ1","executionInfo":{"status":"ok","timestamp":1606357074810,"user_tz":480,"elapsed":46127,"user":{"displayName":"Tooraj Helmi","photoUrl":"","userId":"13522793036712981936"}}},"source":["def predict(data_loader):\n","  model.eval()\n","  predictions , true_labels = [], []\n","\n","  total_loss = 0\n","\n","  for batch in data_loader:\n","    batch = tuple(t.to(device) for t in batch)\n","    b_input_ids, b_input_mask, b_labels = batch\n","\n","    with torch.no_grad():\n","        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels= b_labels)\n","\n","    logits = outputs[1]\n","    total_loss += outputs[0]\n","    logits = logits.detach().cpu().numpy()\n","    label_ids = b_labels.to('cpu').numpy()\n","\n","    predictions.append(logits)\n","    true_labels.append(label_ids)\n","\n","  all_predictions = np.concatenate(predictions, axis=0)\n","  all_true_labels = np.concatenate(true_labels, axis=0)\n","\n","  predicted_label_ids = np.argmax(all_predictions, axis=2)\n","  predicted_label_ids = np.concatenate(predicted_label_ids, axis=0)\n","  all_true_labels = np.concatenate(all_true_labels, axis=0)\n","\n","  real_token_predictions = []\n","  real_token_labels = []\n","\n","  for i in range(len(all_true_labels)):\n","      if not all_true_labels[i] == -100:\n","          real_token_predictions.append(predicted_label_ids[i])\n","          real_token_labels.append(all_true_labels[i])\n","  return real_token_labels, real_token_predictions, total_loss / len(data_loader)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"be80um5ZFdxs"},"source":["## 3.5. Accuracy"]},{"cell_type":"code","metadata":{"id":"G3DguyeGFZQx","executionInfo":{"status":"ok","timestamp":1606357074810,"user_tz":480,"elapsed":46125,"user":{"displayName":"Tooraj Helmi","photoUrl":"","userId":"13522793036712981936"}}},"source":["def acc(y_pred, y_true):\n","  print(y_pred)\n","\n","  _, y_pred_tags = torch.max(y_pred, dim = 2)    \n","  correct_preds = (y_pred_tags == y_true).float()\n","  acc = correct_preds.sum() / torch.numel(y_true)\n","  acc = torch.round(acc * 100)\n","  return acc, y_pred_tags, correct_preds"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b8-RNN5JDxLz"},"source":["## 3.6. Proces Data"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":656},"id":"FUu7Lh8lD-Z5","executionInfo":{"status":"ok","timestamp":1606357696011,"user_tz":480,"elapsed":3349,"user":{"displayName":"Tooraj Helmi","photoUrl":"","userId":"13522793036712981936"}},"outputId":"79977e3b-d36e-4a98-9318-ead9ff60bb03"},"source":["tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","data = load_data(_root_path_ + 'Reqset-comb-tagged.csv', _tag_, True)\n","print(data.head(10))\n","\n","sentences, all_tokens, unique_tokens, labels, unique_labels, label_map = parse_dataset(data)\n","\n","print()\n","print(\"Number of training sentences: {:,}\".format(len(sentences)))\n","print(\"Number of training words: {:,}\".format(len(all_tokens)))\n","print(\"Number of training uniques words: {:,}\".format(len(unique_tokens)))\n","print()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"01GKjJHNoNrg","colab":{"base_uri":"https://localhost:8080/","height":440},"executionInfo":{"status":"ok","timestamp":1605987161363,"user_tz":480,"elapsed":54066,"user":{"displayName":"Tooraj Helmi","photoUrl":"","userId":"13522793036712981936"}},"outputId":"69106095-9444-4a50-c087-3b6eee031f2a"},"source":["if _debug_:\n","  print(\"Example sentence:\")\n","  print (\"    Tokens:\", sentences[18])\n","  print (\"    Labels:\", labels[18])\n","\n","analyze()\n","input_ids, attention_masks = tokenize(sentences)\n","new_labels = map_to_labels()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-yKfkY_ZUxxn"},"source":["## 3.7. Generate Training Data\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j5ju6Npr5kkb","executionInfo":{"status":"ok","timestamp":1605987161364,"user_tz":480,"elapsed":54054,"user":{"displayName":"Tooraj Helmi","photoUrl":"","userId":"13522793036712981936"}},"outputId":"20b323de-6c2f-4232-b1d3-c2da7b51a607"},"source":["from torch.utils.data import TensorDataset, random_split\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","\n","pt_input_ids = torch.stack(input_ids, dim=0)\n","pt_attention_masks = torch.stack(attention_masks, dim=0)\n","pt_labels = torch.tensor(new_labels, dtype=torch.long)\n","\n","dataset = TensorDataset(pt_input_ids, pt_attention_masks, pt_labels)\n","\n","train_size = int(0.9 * len(dataset))\n","val_size = len(dataset) - train_size\n","\n","train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n","\n","train_dataloader = DataLoader(train_dataset, sampler = RandomSampler(train_dataset), batch_size = _batch_size_)\n","validation_dataloader = DataLoader(val_dataset, sampler = SequentialSampler(val_dataset), batch_size = _batch_size_)\n","\n","print('{:>5,} training samples'.format(train_size))\n","print('{:>5,} validation samples'.format(val_size))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8bwa6Rts-02-"},"source":["# Train"]},{"cell_type":"markdown","metadata":{"id":"qRWT-D4U_Pvx"},"source":["## 4.1. Define Model\n","\n"]},{"cell_type":"code","metadata":{"id":"-p0upAhhRiIx","colab":{"base_uri":"https://localhost:8080/","height":220,"referenced_widgets":["ef65b34c044249898b0b86278175367b","e422e09a72564a29a3ea61a61c011faf","57aab739f6754331b15934c19f1c7b94","8460afa17a2645c5bba53405731885f3","5ff0e7a945a846668072388c05eeae62","f72b12d079a74e8086db8d56edf7917b","bc7cff93cabc4e7fa68ca4e9723a6ccf","fa84b8ae16be4517a497fb7f56396f88","6087355cfc5c4e5f832c3d50ec03e92c","ca03d06bc88644cc89175c0e692365c6","5065180a3fce4e788ee6d3e193b655ca","41980f25ce5e485cb1b7a3c37d0db018","0cdb153fffb04401af6be8bc3b49a190","3a5b972f4616451eb150ecf0b13f1033","f32641cac54d45ec8ead8201f4d1aaa8","6fb8cb633526461183b57a51794716c8"]},"executionInfo":{"status":"ok","timestamp":1605987182359,"user_tz":480,"elapsed":75037,"user":{"displayName":"Tooraj Helmi","photoUrl":"","userId":"13522793036712981936"}},"outputId":"b25ef385-a656-4a5a-a9bf-6f6a15292d5f"},"source":["from transformers import BertForTokenClassification, AdamW, BertConfig\n","from transformers import get_linear_schedule_with_warmup\n"," \n","model = BertForTokenClassification.from_pretrained(\n","    \"bert-base-uncased\", \n","    num_labels = len(label_map) + 1, \n","    output_attentions = False, \n","    output_hidden_states = False,\n","    # return_dict = True\n",")\n","\n","model.cuda()\n","\n","optimizer = AdamW(model.parameters(), lr = _learning_rate_)\n","total_steps = len(train_dataloader) * _epochs_\n","scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = total_steps)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RqfmWwUR_Sox"},"source":["## 4.2. Execute Training"]},{"cell_type":"code","metadata":{"id":"6J-FYdx6nFE_","colab":{"base_uri":"https://localhost:8080/","height":406,"referenced_widgets":["e0c2db4225b1480f88c87f56bb709d40","9564a352a0e34f10935db3e6b9c3fa3e","2a5ed12259c44522b11521cd2a1ef45a","3165271833e146d2945567a3b265d346","dd2a993fb55845a6a012873a02ceae39","f07a9f58a3c44c75b5aeb56ca9f81513","9dabe991be8c4d188e52e3029c9524bb","dc24c8da5c2b4ae4bdecc0e2e83ec517"]},"executionInfo":{"status":"ok","timestamp":1605987194746,"user_tz":480,"elapsed":87413,"user":{"displayName":"Tooraj Helmi","photoUrl":"","userId":"13522793036712981936"}},"outputId":"d520606b-0abd-4ad9-c1b0-39d2336321f5"},"source":["import random\n","from tqdm.notebook import tqdm\n","from seqeval.metrics import f1_score, accuracy_score\n","\n","seed_val = 42\n","\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","train_stats = {\n","    'acc': [],\n","    'loss': []\n","}\n","\n","val_stats = {\n","    'acc': [],\n","    'loss': [],\n","    'f1': []\n","}\n","\n","for e in tqdm(range(1, _epochs_+1)):   \n","    model.train()\n","    train_epoch_loss = 0\n","    train_epoch_acc = 0\n","\n","    batch_idx = 0\n","    for train_batch in train_dataloader:\n","        batch_idx += 1\n","        if batch_idx % 10 == 0:\n","          print(\".\", end =\"\") \n","        if batch_idx % 1000 == 0:\n","          print();\n","\n","        t_input_ids = train_batch[0].to(device)\n","        t_input_mask = train_batch[1].to(device)\n","        t_labels = train_batch[2].to(device)\n","\n","        model.zero_grad()        \n","        t_outputs = model(t_input_ids, token_type_ids=None, attention_mask=t_input_mask, labels=t_labels)  \n","        \n","        train_epoch_loss += t_outputs[0].item() \n","\n","        t_outputs[0].backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","        optimizer.step()\n","        scheduler.step()\n","\n","    # VALIDATION    \n","    with torch.no_grad():      \n","      true_tags, pred_tags, val_loss = predict(validation_dataloader)\n","\n","      val_acc = accuracy_score(pred_tags, true_tags)\n","      # val_f1 = f1_score(pred_tags, true_tags)\n","\n","      train_stats['loss'].append(train_epoch_loss/len(train_dataloader))\n","      val_stats['loss'].append(val_loss.item())\n","      val_stats['acc'].append(val_acc.item())\n","      # val_stats['f1'].append(val_f1/len(validation_dataloader))\n","            \n","      print(f'Epoch {e+0:03}: | Train Loss: {train_epoch_loss/len(train_dataloader):.5f} | Val Loss: {val_loss:.5f} | Val Acc: {val_acc:.3f}') #' | Val F1: {val_f1/len(validation_dataloader): .3f}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":728},"id":"PmNw8-2Dgfn_","executionInfo":{"status":"ok","timestamp":1605987195758,"user_tz":480,"elapsed":88412,"user":{"displayName":"Tooraj Helmi","photoUrl":"","userId":"13522793036712981936"}},"outputId":"330d26be-1fba-4e79-aceb-355d06d28376"},"source":["import matplotlib.pyplot as plt\n","\n","hist = pd.DataFrame(list(zip(val_stats['acc'])), columns=['val-acc'], dtype=float) \n","plt.style.use(\"ggplot\")\n","plt.figure(figsize=(12, 12))\n","# plt.plot(hist['train-acc'])\n","plt.plot(hist['val-acc'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zBhywMlXVmUJ","colab":{"base_uri":"https://localhost:8080/","height":427},"executionInfo":{"status":"ok","timestamp":1605987196184,"user_tz":480,"elapsed":88825,"user":{"displayName":"Tooraj Helmi","photoUrl":"","userId":"13522793036712981936"}},"outputId":"b21a7d43-495c-4d1f-c40b-55d13f2ef0f9"},"source":["import matplotlib.pyplot as plt\n","% matplotlib inline\n","\n","import seaborn as sns\n","\n","# Use plot styling from seaborn.\n","sns.set(style='darkgrid')\n","\n","# Increase the plot size and font size.\n","sns.set(font_scale=1.5)\n","plt.rcParams[\"figure.figsize\"] = (12,6)\n","\n","# Plot the learning curve.\n","# plt.plot(loss_values, 'b-o')\n","\n","# Label the plot.\n","plt.title(\"Training loss\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mkyubuJSOzg3"},"source":["# Test"]},{"cell_type":"markdown","metadata":{"id":"30w5nU1aY4Pp"},"source":["## 5.1. Generate Testing Data\n"]},{"cell_type":"code","metadata":{"id":"CwVgVsvNm3a3","colab":{"base_uri":"https://localhost:8080/","height":719},"executionInfo":{"status":"ok","timestamp":1605987550739,"user_tz":480,"elapsed":1545,"user":{"displayName":"Tooraj Helmi","photoUrl":"","userId":"13522793036712981936"}},"outputId":"8b521956-76a9-43c7-f375-093f5ab9d9e4"},"source":["data = load_data(_root_path_ + 'reqset_test_tagged.csv', _tag_, True)\n","print(data.head(10))\n","\n","sentences, all_tokens, unique_tokens, labels, unique_labels, _ = parse_dataset(data)\n","input_ids, attention_masks = tokenize(sentences)\n","new_labels = map_to_labels()\n","\n","pt_input_ids = torch.stack(input_ids, dim=0)\n","pt_attention_masks = torch.stack(attention_masks, dim=0)\n","pt_labels = torch.tensor(new_labels, dtype=torch.long)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"16lctEOyNFik"},"source":["## 5.2. Evaluate \n"]},{"cell_type":"code","metadata":{"id":"wM_KID9aIZj6","colab":{"base_uri":"https://localhost:8080/","height":854},"executionInfo":{"status":"ok","timestamp":1605988731929,"user_tz":480,"elapsed":2449,"user":{"displayName":"Tooraj Helmi","photoUrl":"","userId":"13522793036712981936"}},"outputId":"53c979b7-d530-457d-e46f-5ab63008012c"},"source":["from sklearn.metrics import f1_score\n","from sklearn.metrics import confusion_matrix, classification_report\n","\n","batch_size = 1\n","test_data = TensorDataset(pt_input_ids, pt_attention_masks, pt_labels)\n","test_sampler = SequentialSampler(test_data)\n","test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n","true_labels, pred_labels, _ = predict(test_dataloader)\n","\n","cm = confusion_matrix(true_labels, pred_labels)\n","confusion_matrix_df = pd.DataFrame(cm)\n","\n","label_names = (np.asarray([l for l in label_map]))\n","fig, ax = plt.subplots(figsize=(15,13))  \n","sns.heatmap(confusion_matrix_df, annot=True, ax=ax, fmt='g', xticklabels=label_names, yticklabels=label_names)\n","plt.yticks(rotation=0) \n","# print(true_labels)\n","# print(pred_labels)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gTHnjaGPhDOw","colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"status":"ok","timestamp":1605987198356,"user_tz":480,"elapsed":90966,"user":{"displayName":"Tooraj Helmi","photoUrl":"","userId":"13522793036712981936"}},"outputId":"dc814a30-9cab-4ec7-a353-282b193f750b"},"source":["%%HTML\n","<style type=\"text/css\">\n","table.dataframe td, table.dataframe th {\n","    border: 1px  black solid !important;\n","  color: black !important;\n","}\n","</style>"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1q7ggFNaTkWu","colab":{"base_uri":"https://localhost:8080/","height":379},"executionInfo":{"status":"ok","timestamp":1605987198357,"user_tz":480,"elapsed":90954,"user":{"displayName":"Tooraj Helmi","photoUrl":"","userId":"13522793036712981936"}},"outputId":"8e916bc0-3d5a-406e-8484-fb064b039e55"},"source":["from sklearn.metrics import f1_score\n","from sklearn.metrics import precision_recall_fscore_support as score\n","\n","precision, recall, fscore, support = score(true_labels, pred_labels, labels=[v for v in label_map.values()])\n","\n","f1_micro = f1_score(true_labels, pred_labels, average='micro') \n","f1_macro = f1_score(true_labels, pred_labels, average='macro') \n","f1_weighted = f1_score(true_labels, pred_labels, average='weighted') \n","\n","print (\"Micro F1 score: {:.2%}\".format(f1_micro))\n","print (\"Macro F1 score: {:.2%}\".format(f1_macro))\n","print (\"Weighted F1 score: {:.2%}\".format(f1_weighted))\n","\n","df = pd.DataFrame({'labels': [k for k in label_map.keys()], 'precision': precision, 'recall': recall, 'fscore': fscore, 'support': support})\n","df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZevRAlfUkbdd","executionInfo":{"status":"ok","timestamp":1605989829266,"user_tz":480,"elapsed":1104,"user":{"displayName":"Tooraj Helmi","photoUrl":"","userId":"13522793036712981936"}},"outputId":"d169a210-da72-4a3e-e366-f8bd3e9f740f"},"source":["words = [\"if\", \"user\", \"picks\", \"the\", \"first\", \"choice\", \",\", \"he\", \"should\", \"be\", \"shown\", \"a\", \"white\", \"balloon\", \"below\", \"the\", \"text\"]\n","sent = [1 for i in range(len(words))]\n","tags = [\"-\" for i in range(len(words))]\n","data = pd.DataFrame(list(zip(sent, words, tags, tags)), columns=[\"SENT\", \"WORD\", \"POS\", _tag_])\n","\n","sentences, all_tokens, unique_tokens, labels, unique_labels, _ = parse_dataset(data, False)\n","input_ids, attention_masks = tokenize(sentences)\n","new_labels = map_to_labels()\n","\n","pt_input_ids = torch.stack(input_ids, dim=0).to(device)\n","pt_attention_masks = torch.stack(attention_masks, dim=0).to(device)\n","\n","with torch.no_grad():\n","  outputs = model(pt_input_ids, token_type_ids=None, attention_mask=pt_attention_masks)\n","logits = outputs[0]\n","logits = logits.detach().cpu().numpy()\n","predicted_label_ids = np.argmax(logits, axis=2)[0]\n","\n","untokenized = [tokenizer.ids_to_tokens[t.numpy().item()] for t in input_ids[0]]\n","\n","for i, w in enumerate(words):\n","  label_id = predicted_label_ids[i+1]\n","  label = ''\n","  for l in label_map:\n","    if label_map[l] == label_id:\n","      label = l\n","      break\n","  print('{:>20}: '.format(untokenized[i+1]), label)"],"execution_count":null,"outputs":[]}]}